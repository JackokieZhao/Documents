# 概述

本节介绍YOLO v3的基本原理和实现。YOLO v3是一种快速的目标检测算法，我们将使用PyTorch实现该算法。

> 本节的大部分内容来源于Ayoosh Kathuria在Medium发表的系列文章，但有部分内容是编译时的扩充内容。原作者是印度国防研究组织的实习生，他的研究方向是模糊视频的目标检测。可以通过GitHub关注他的最新研究动态：https://github.com/ayooshkathuria
>
> 原文参见：https://medium.com/paperspace/tutorial-on-implementing-yolo-v3-from-scratch-in-pytorch-part-1-a0054d38ec78

本节包括以下五个部分的内容：

- YOLO的工作原理
- 创建YOLO网络架构
- 网络的前向传播
- Objectness置信度阈值和非极大值抑制
- 设计输入和输出管道

# YOLO的基本原理

## 什么是YOLO

YOLO是You Only Look Once的缩写，算法的命名致敬了乔布斯的名言：

> You only live once . Make it count.

它是一种使用深度卷积神经网络得到的特征来进行目标检测的算法，并在各类测试数据集和实际应用中取得了极佳的效果，是目前最优秀的目标检测算法之一。

## 全卷积神经网络（FCN）

YOLO仅使用卷积层，这使其成为全卷积神经网络（FCN）。它拥有75个卷积层，还有跳过连接（skip connections ）和上采样层（upsampling layers）。不适用任何的池化，而采用步幅为2的卷积层对特征图进行下采样，这样有助于防止池化导致的低级特征丢失。

由于采用FCN的结构，YOLO对输入图像的尺寸不敏感，而实际应用中，我们仍会采用固定不变的输入大小。其原因在于：如果我们希望按batch处理数据，则需要固定图像的宽度和高度。一般通过张量合并实现这一点。

YOLO通用stride因子来进行下采样。例如，如果网络的stride为32，则大小为416x416的输入图像将产生13x13的输出。通常，网络任意一层的stride等于输入的大小除以这一层输出的大小。

## 特征图

目标检测算法通常把卷积层学习的特征传递到分类/回归器，从而进行边界和类别的预测。

在YOLO中，预测是通过卷积层完成的，其卷积核（kernel）的尺寸为：
$$
1 \times 1 \times (B \times (5 + C))
$$
注意，我们的输出是一个特征图（feature map）。由于我们使用1x1卷积，预测图（prediction map）的大小刚好是特征图的大小。在YOLO v3中，预测图就是可以预测固定数量边界框（bounding box）的单元格（cell）。

> 严格的说，应该使用神经元（neuron）这一术语，但是这里称为单元格显得更加直观。

在深度层面，特征图有(Bx(5+C))项。其中：B代表每个单元格可以预测的边界框数量，B个边界框中的每一个都可能专门用于检测某种对象；C是类别数量，每个边界框都有5+C个属性，分别描述每个边界框的中心坐标、维度、Objectness分数和C类置信度。YOLO v3在每个单元格中预测3个边界框。

这里首先介绍几个概念：

- 特征图（feature map）：是输入图像通过卷积核（或者称为Filter）进行卷积操作得到的，一个卷积核对应了卷积后得到的一个特征图，不同卷积核卷积后得到不同特征图，用于提取不同特征，得到不同的specialized neuro。
- 感受野（receptive field）：某一层输出结果中一个单元格对应的输入层的区域大小，即输出层的单元格对应输入层的一个映射，或者说特征图上的一个点对应输入图上的区域。感受野的大小计算参见下图：

![dlpractice](D:/Documents/images/dlpractice_01_1.png)

如果对象的中心位于单元格感受野内，那么我们希望特征图中的每个单元格都可以通过它的一个边界框来预测对象。在YOLO中只有一个边界框负责检测任意给定的目标/物体。首先我们必须查明这个边界框到底属于哪个单元格。为了做到这一点，我们把输入图像划分成了一系列网格，网格的维度等于最后一个特征图的维度。

接下来我们通过一个简单的例子来说明。输入图像的大小是416x416px，步长（stride）为32。如前所述，特征度的大小将会是13x13。我们把整张图划分成13x13个单元格。

![dlpractice](D:/Documents/images/dlpractice_01_2.png)

输入图像上的那个红色的单元格包含了检测目标真实的边界框（黄色的框）的中心点。现在，我们认为将特征图上这个红色的单元格对应的位置视为检测原始图像中的狗狗的可靠位置。前面提到，一个单元格可以预测3个边界框。到底把那个分配给狗狗的预测框呢？这里又需要引入锚点（anchors）的概念。

> 我们这里讨论的单元格是指预测特征图上的单元格。我们将输入图像划分为网格，只是为了确定预测特征图上的哪一个单元格对预测是有贡献的。

## 锚点框（Anchor Boxes）

预测边界框的宽度和高度是非常有意义的，但是实际操作中，这将会使训练过程中的梯度不稳定。相反，大多数现代的目标检测算法都是预测对数空间的变换（log-space transforms），或者更加简单地——预定义的默认的边界框的平移。这种预定义的边界框被称为**锚点**（anchors）。锚点可以理解为“候选区域形状”，例如下图示出了9中锚点：

![dlpractice](D:/Documents/images/dlpractice_01_3.png)

这些变换（比如平移）被应用到锚点框来得到预测。YOLO v3有3个锚点，也就是说每一个单元格都能够预测3个边界框。

我们回到之前的问题。预测狗狗的边界框将会是和真正的位置标记框（ground truth box）有着最高的IoU的锚点。

> IoU是Intersection-over-Union的缩写，即“交并比”，其概念是产生的候选框和原标记框的交叠率，即他们的交集与并集的比值，计算公式是：
> $$
> IoU = \frac {area(C) \cap area(G)} {area(C) \cup area(G)}
> $$
> 式中，C和G分别是候选框和原标记框。

## 预测

以下公式描述了网络的输出是如何转换成预测的边界框的：
$$
b_x = \sigma(t_x) + c_x
$$

$$
b_y = \sigma(t_y) + c_y
$$

$$
b_w = p_w e^{t_w}
$$

$$
b_h = p_h e^{t_h}
$$

以上各式中，(bx, by)是中心点坐标，而bw、bh分别为边界框的宽度和高度；(tx, ty)是直接输出结果，即一个相对的、正则化的坐标，(cx, cy)是单元格的左上角点；tw、th是对边界框宽度和高度的预测输出，得到最终输出需要乘上锚点，即pw、ph。详细原理将在下文解释。

## 中心点坐标

我们用一个sigmoid函数来获得中心点坐标。这使得输出值的范围限定在0到1之间。一般而言，YOLO并不预测边界框中心点的绝对坐标，而是预测偏移，这种偏移是相对于预测目标的单元格的左上角点的，并且被特征图中的单元格维度正则化。比如说，在之前预测狗狗位置的例子中，如果中心点的预测值为(0.4, 0.7)，那么由于红色的单元格的左上角点坐标为(6, 6)，中心点在13x13的特征图中的绝对坐标是(6.4, 6.7)。

但如果预测的值出现了大于1的数，例如(1.2, 0.7)，这意味着中心点在(7.2, 6.7)，不在红色单元格内了，从而违背了YOLO的原理，因为如果我们假定红色单元格是用于预测狗狗位置的，那么中心点坐标一定位于红色单元格内。因此，为了解决这个问题，中心点坐标被传递到了sigmoid函数中，强制限定输出范围为0~1。

## 边界框的大小

边界框大小是通过对输出进行对数空间变换、并乘一个锚点来进行预测的。下图可以辅助理解从网络输出到最终预测结果的过程。

![dlpractice](D:/Documents/images/dlpractice_01_4.png)

预测的最终输出结果bw和bh，被图片的宽和高正则化。所以，如果bx和by是(0.3, 0.8)，那么在13x13的特征图上，真实宽度和高度是(13x0.3, 13x0.8)。

## Objectness分数（Objectness Score）

目标检测分数表示在一个边界框内的目标物体的预估概率，越接近1表明越靠近物体的中心点，越接近0则表明越远离物体的中心点。这个分数也被传递给sigmoid函数，从而限定在0~1的区间内。

## 类别置信度（Class Confidences）

类别置信度表示检测到的物体属于某个特定类别的概率。在v3之前，YOLO一直使用softmax类别分数来获得置信度。然而，在v3版本中抛弃了这种做法，取而代之的是sigmoid函数。原因是softmax使用的前提是类别之间是互斥的（mutually exclusive）。简单地说，如果一个物体属于某一类，那么它一定不属于另一类。这对于COCO数据集而言是没有问题的。但是在实际应用中，很多时候我们的分类可能是相互重叠的，例如“女性”和“人类”。

## 在不同大小比例下进行预测

YOLO v3在3种不同大小比例下进行预测。检测层（detection layer）用于在特征图上采取不同的步长值进行检测，具体而言，将会使用32, 16, 8三种步长进行检测。这意味着，如果原始图片的输入是416x416，那么我们将在13x13、26x26、52x52三种特征图上进行目标检测。

网络对输入图像进行下采样，直到第一个检测层，即步长为32的检测层。进一步地，网络将以2为因子进行上采样并于有相同大小的特征图的前一个层的特征图进行连接。下一次检测的步长为16，之后重复上采样过程。最后以8为步长进行检测。

在每一种大小比例下，每个单元格用3个锚点预测3个边界框，如此一来，锚点框综述为9（不同大小比例下，锚点框是不同的）。

![dlpractice](D:/Documents/images/dlpractice_01_5.png)

YOLO的作者声称，v3版本的算法对小物体能够获得更好的检测效果。上采样能够帮助网络学到更多细粒度的特征，这对检测小物体是非常有帮助的。

## 输出处理

对于一张416x416的图片，YOLO预测的边界框总数为：
$$
((52 \times 52) + (26 \times 26) + (13 \times 13)) \times 3 = 10647
$$
但是实际上，我们的图片中只有一个物体——一只狗。我们怎样将10647减少为1呢？

### 目标置信度阈值

首先，我们基于Objectness分数过滤一部分边界框，小于一定阈值的边界框将会被过滤掉。

### 非极大值抑制（Non-maximum Suppression）

NMS用于解决同一张图片的多重检测问题。例如，下图中以紫色的单元格为中心都检测到了同一目标，那么实际上我们可以只保留其中一个。

> 非极大值抑制，顾名思义就是抑制不是极大值的元素。可以理解为局部最大搜索。首先我们找到置信度最高的边界框B1，然后计算其余边界框与该边界框的IoU。我们去掉IoU大于某阈值的边界框，同时保留B1。之后从得以保留的边界框（除了B1外）中，找到最大置信度的边界框，再次重复以上过程，直到候选的边界框数量为0。这样一来，我们就找到了置信度高，而IoU比较小的边界框。

![dlpractice](D:/Documents/images/dlpractice_01_6.png)

## 算法实现

我们将使用官方给出的YOLO网络权重文件作为基准模型。这些权重是通过训练COCO数据集来得到的，因此可以检测80个类别的物体。以下各小节将会详细描述算法的实现。

# 创建网络结构

## Getting Started

首先创建用于存储所有相关文件的目录。然后创建`darknet.py`文件。Darknet是YOLO依赖的网络结构的名称。这个文件将包含YOLO网络的创建代码。我们将用`util.py`文件存储若干辅助函数。

## 配置文件

官方版本的YOLO是使用C语言编写的。`cfg`文件用于描绘整个网络的结构。这类似于Caffe的`.protxt`配置文件。我们这里使用官方提供的`cfg`文件，下载地址：https://github.com/pjreddie/darknet/raw/master/cfg/yolov3.cfg

我们将`cfg`文件放在`cfg`文件夹。打开该文件将会看到以下内容：

```
[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[shortcut]
from=-3
activation=linear
```

这里包含了4个block的定义，其中3个为卷积层，另外1个为shortcut，也就是所谓skip connection。Skip connection跳过了某些block而直接与后面的模块相连，很多极深的网络（例如ResNet）都采用了这种结构。

YOLO的配置文件定义了以下几种网络结构：

- Convolutional：卷积层

  ```
  [convolutional]
  batch_normalize=1
  filters=64
  size=3
  stride=1
  pad=1
  activation=leaky
  ```

- Shortcut：跳过连接

  ```
  [shortcut]
  from=-3
  activation=linear
  ```

  其中的`from`参数表示shortcut层是通过将后面的第三层加到特征图中得到的。

- Upsample：上采样

  ```
  [upsample]
  stride=2
  ```

  以指定的步长进行双线性上采样。

- Route

  ```
  [route]
  layers = -4

  [route]
  layers = -1, 61
  ```

  Route层中的`layers`可以有一个或两个值。当只有一个值时，它输出使用该值进行索引的层的特征图。例如这里的值-4表示输出Route层后面的第4层的特征图。当有两个值时，返回这两个值索引的链接特征图。例如这里的-1, 61表示该层将输出从上一层到第61层的特征图。

- YOLO

  ```
  [yolo]
  mask = 0,1,2
  anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
  classes=80
  num=9
  jitter=.3
  ignore_thresh = .5
  truth_thresh = 1
  random=1
  ```

  指检测层。`anchors`包含了9种锚点框（预选框），但是只有mask标记的才会被使用。由于每个单元格预测3个边界框，所以这里`mask`设为3。我们一共涉及了3种大小比例，从而组成9种锚点。

- Net

  ```
  [net]
  # Testing
  batch=1
  subdivisions=1
  # Training
  # batch=64
  # subdivisions=16
  width= 320
  height = 320
  channels=3
  momentum=0.9
  decay=0.0005
  angle=0
  saturation = 1.5
  exposure = 1.5
  hue=.1
  ```

  该模块描述了网络的输入和训练参数。

## 配置文件解析

首先导入必要的模块：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable  # 仅PyTorch 0.3以前的版本需要
import numpy as np
```

我们定义一个`parse_cfg`函数，其参数为配置文件路径。解析配置文件时，每个block都会被存储为一个dict。所有的block都会存放在一个list中。

```python
def parse_cfg(cfgfile):
    """
    导入配置文件

    返回一个block列表。每个block描述了神经网络的一部分结构。Block用list中的dict表示。
    """
    file = open(cfgfile, 'r')
    lines = file.read().split('\n')  # 每行存储为一个list元素
    lines = [x for x in lines if len(x) > 0]  # 去掉空行
    lines = [x for x in lines if x[0] != '#']  # 去掉注释
    lines = [x.rstrip().lstrip() for x in lines]  # 取消边缘的空格

    block = {}
    blocks = []
    for line in lines:
        if line[0] == "[":  # 这意味着是一个新的block
            if len(block) != 0:  # 如果非空，意味着它存储了上一个block的信息
                blocks.append(block)  # 加入列表
                block = {}  # 清空block信息
            block["type"] = line[1:-1].rstrip()  # block的类型
        else:
            key, value = line.split("=")
            block[key.rstrip()] = value.lstrip()
    blocks.append(block)

    return blocks
```

## 创建block

接下来我们将使用`parse_cfg`返回的list来创建PyTorch模块。在YOLO包含的模块中，`convolutional`和`upsample`是PyTorch内置的模块，使用`nn`就可以创建它们。而其他模块需要我们自己编写，这是通过扩展`nn.Module`类实现的。

下面的`create_modules`函数将把block的list作为输入，然后创建它们：

```python
def create_modules(blocks):
    net_info = blocks[0]  # 获取关于网络的一些基本信息，例如输入和预处理的方法
    module_list = nn.ModuleList()
    prev_filters = 3
    output_filters = []
```

### nn.ModuleList

函数返回`nn.ModuleList`，这是一个包含`nn.Module`对象列表的类。然而，当我们向网络中添加模块时，所有网络中子模块`nn.Module`对象的`parameter`s也都将被添加作为我们的整个网络`nn.Module`对象的参数。

当我们定义一个新的卷积层时，必需定义其核的维度。卷积核的宽度和高度已经在`cfg`文件中提供了，而卷积核的深度是上一层filter的数量（或者特征图的深度）。这意味着，我们需要记录被卷积的层的filter数量。我们用`prev_filter`来实现这一点。由于图像有3个filter（即RGB三个通道），我们将其初始化为3。

Route层从之前的层中获得特征图。如果Route层前面恰好有一个卷积层，卷积核将用在之前各层的特征图上。因此，我们不仅需要记录上一层的filter数量，还要存储之前每一层的filter数量。在迭代过程中，我们将每个block输出filter的数量添加到`output_filters`列表中。

现在，我们开始创建PyTorch模块：

```python
for index, x in enumerate(blocks[1:]):
    module = nn.Sequential()

    # 检查block的类型
    # 创建一个新的模块
    # 将其添加到module_list中
```

`nn.Sequential`类用于顺序执行一系列`nn.Module`对象。一个block可能包含不止一个层，例如`convolutional`block可能有很多正则化层和leaky ReLU激活层以及一个卷积层。所以我们统一使用`nn.Sequential`来处理这种情况。下面我们首先创建卷积层和上采样层，这两种模块是PyTorch自带的：

```python
if x['type'] == 'convolutional':  # 卷积层
    # 获取层的信息
    activation = x['activation']
    try:
        batch_normalize = int(x['batch_normalize'])  # 是否批正则化
        bias = False
    except:
        batch_normalize = 0
        bias = True

    filters = int(x['filters'])  # 卷积核数量
    padding = int(x['pad'])  # 是否padding（填充空缺）
    kernel_size = int(x['size'])  # 卷积核大小
    stride = int(x['stride'])  # 步长

    if padding:
        pad = (kernel_size - 1) // 2  # 两个斜线表示除法取整数部分，注意此处pad的计算方法
    else:
        pad = 0

    # 添加一个卷积层
    conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)
    module.add_module("conv_{}".format(index), conv)

    # 添加一个批正则化层
    if batch_normalize:
        bn = nn.BatchNorm2d(filters)
        module.add_module("batch_norm_{0}".format(index), bn)

    # 激活函数（线性或者Leaky ReLU）
    if activation == "leaky":
        activn = nn.LeakyReLU(0.1, inplace=True)
        module.add_module("leaky_{}".format(index), activn)

elif x['type'] == 'upsample':  # 上采样层
    stride = int(x['stride'])
    upsample = nn.Upsample(scale_factor=2, mode='bilinear')  # 双线性二维上采样
    module.add_module('upsample_{}'.format(index), upsample)
```

### Route层/Shortcut层

这两种模块是PyTorch没有定义的，所以我们自己实现它们：

```python
elif x['type'] == 'route':  # Route层
    x['layers'] = x['layers'].split(',')  # 可能有一个值，也可能有两个
    start = int(x['layers'][0])
    try:
        end = int(x['layers'][1])
    except:
        end = 0
    if start > 0:
        start = start - index  # index是block的编号
    if end > 0:
        end = end - index
    route = EmptyLayer()  # EmptyLayer
    module.add_module("route_{0}".format(index), route)
    # 以下代码更新filters变量来存储Route层输出的filter数量：
    if end < 0:  # layers给出两个值的情况
        filters = output_filters[index + start] + output_filters[index + end]
    else:  # layers给出一个值的情况
        filters = output_filters[index + start]

elif x['type'] == 'shortcut':  # 跳过连接层
    shortcut = EmptyLayer()
    module.add_module("shortcut_{}".format(index), shortcut)
```

注意，我们创建Route层时使用的是`EmptyLayer`，写入以下代码创建它：

```python
class EmpytLayer(nn.Module):
    def __init__(self):
        super(EmptyLayer, self).__init__()
```

### Empty层

从Empty的创建代码中，我们可以看出它没有做任何事情。但事实上，Route层确实跟其他层一样做了某些操作。在PyTorch中，当我们定义一个新的层时，需要继承`nn.Module`并将操作写在`forward`函数中。但是由于连接的代码如此简短（我们可以在特征图调用`torch.cat`），向`forward`中添加代码是没有必要的。取而代之，我们用一个虚拟的层代替Route层，之后直接在表示darknet的`nn.Module`的`forward`函数中执行操作即可。

Shortcut层同样适用empty层，因为其进行的操作也很简单。我们不必更新`filters`变量，因为它从不将上一层的特征图直接添加到下面一层。

### YOLO层

最后我们编写YOLO层。YOLO层时darknet中进行目标检测的层。

```python
elif x['type'] == 'yolo':  # YOLO层
    mask = x['mask'].split(',')
    mask = [int(x) for x in mask]
    anchors = x['anchors'].split(',')
    anchors = [int(a) for a in anchors]
    anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors), 2)]  # 还原tuple
    anchors = [anchors[i] for i in mask]  # 可用的anchor

    detection = DetectionLayer(anchors)
    module.add_module("Detection_{}".format(index), detection)
```

其中的`DetectionLayer`定义如下：

```python
class DetectionLayer(nn.Module):
    def __init__(self, anchors):
        super(DetectionLayer, self).__init__()
        self.anchors = anchors  # 存储可用的anchor
```

在循环结束前，我们进行存储记录：

```python
module_list.append(module)
prev_filters = filters
output_filters.append(filters)
```

最后，返回`net_info`和`module_list`：

```python
return (net_info, module_list)
```

## 代码测试

我们可以在`darknet.py`文件的最后添加测试代码并运行：

```python
blocks = parse_cfg("cfg/yolov3.cfg")
print(create_modules(blocks))
```

结果将是一个长列表，其中包括了整个网络的所有模块。

未完待续……
