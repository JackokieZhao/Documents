# What is PyTorch?

PyTorch是Python科学计算库，它的主要功能是：

- 代替NumPy，并充分利用GPU来加速计算；

- 作为灵活而高效的深度学习研究工具。

——注意，PyTorch的定位首先是一个能利用GPU计算资源的通用计算库，而不是深度学习库。

## 基本概念

### Tensor

Tensor类似于NumPy中的ndarray，同时可以利用GPU来加速计算。

例如，我们建立一个5x3的矩阵，不进行任何初始化：

```python
from __future__ import print_function
import torch
x = torch.empty(5, 3)
print(x)
```

于是，我们得到了一个5行、3列的二维Tensor，通常情况下大多数的元素为0，但是某些位置可能是一些随机的值，这些值有可能过大，以至于出现RuntimeError: Overflow when unpacking long的错误。当运行出错时，可以重新生成一次。

我们还可以生成一个随机初始化的Tensor：

```python
x = torch.rand(5, 3)
print(x)
```

采用`torch.rand`初始化时，Tensor的每个元素随机初始化为0~1之间的值。

也可以用`torch.zero`将Tensor初始化为0，同时我们可以用dtype参数来设置Tensor元素的数据类型，这里指定为长整型，我们可以看到输出的结果是没有小数部分的：

```python
x = torch.zero(5, 3, dtype=torch.long)
print(x)
```

Tensor还可以直接从list转换，对应的方法为`torch.tensor`：

```python
x = torch.tensor([5,5, 3])
print(x)
```

或者，我们可以利用已有的Tensor来创建新的Tensor：

```python
x = x.new_ones(5, 3)
print(x)
```

x是一个已存在的Tensor，通过调用它的`new_*`方法，可以生成新的Tensor，`new_*`方法的参数是Tensor的尺寸。这里的`new_ones`方法生成的是元素全为1的Tensor，其各种属性与原Tensor相同，例如，如果原来x的元素数据类型为torch.long，那么新的Tensor的元素数据类型也为torch.long。

类似的方法还有：

```python
x = torch.randn_like(x, dtype=torch.float)
print(x)
```

这里基于x生成了一个新的随机初始化的Tensor，但是覆盖了原来的dtype，输出的结果与原Tensor有相同的尺寸。

用Tensor的`size`方法可以查看尺寸：

```python
print(x.size())
```

返回的size数据类型为tensor.Size，其本质上是tuple。

总结一下，Tensor的创建有以下几种方法：

```python
x = torch.empty(5, 3)  # 创建Tensor，不进行初始化，未初始化的Tensor，print出来没有任何意义
y = torch.rand(5, 3)  # 创建用0~1之间的随机数初始化的Tensor
z = torch.zero(5, 3)  # 创建0初始化的Tensor
m = torch.ones(5, 3)  # 创建1初始化的Tensor
n = torch.tensor([5.5, 3])  # 从list创建Tensor
p = x.new_ones(5, 3)  # Tensor的new_*方法用于利用原Tensor的属性创建新Tensor，用size做参数
q = torch.randn_like(x)  # *_like方法利用原Tensor的属性创建Tensor，且size与原Tensor相同
```

用Tensor的`size`方法可以查看其尺寸。

### Operation

Tensor可以用于进行多种运算。同种运算可能有不同的语法来实现。我们以加法操作为例：

- 加法语法1——直接使用+运算符：

  ```python
  y = torch.rand(5, 3)
  print(x + y)
  ```

- 加法语法2——使用`add`方法：

  ```python
  print(torch.add(x, y))
  ```

- 加法语法3——为`add`提供一个输出结果的Tensor：

  ```python
  result = torch.empty(5, 3)  # 只是一个用于输出结果的Tensor
  torch.add(x, y, out=result)  # 计算结果将输出到result中
  print(tensor)
  ```

- 加法语法4——原位相加：

  ```python
  y.add_(x)
  print(y)
  ```

> **注意**：
>
> 所有的原位运算符，都有一个后缀`_`，例如`x.copy_(y)`、`x.t_()`，这些方法将会直接改变`x`的值。

我们可以用类似于NumPy的索引方法对Tensor进行索引：

```python
print(x[:, 1])
```

如果希望能改变Tensor的尺寸，可以使用`torch.view`方法：

```python
x = torch.randn(4, 4)
y = x.view(16)  # 一个长度为16的向量
z = x.view(-1, 8)  # -1表示自动推断该维度的尺寸
print(x.size(), y.size(), z.size())
```

修改尺寸时，务必注意修改前后Tensor的元素总数是相等的，否则会报错。

Tensor是一种特殊的对象，但是可以返回某个元素的Python数值。如果有一个标量Tensor，使用`.item()`方法能够获取在Python中的数值：

```python
x = torch.randn(1)
print(x.item())  # 只有零维标量能使用该方法
print(x[0, 0].item())  # Tensor中的一个单独的元素就是一个零维标量
```

这里介绍了几种常用的运算操作：

```python
x + y  # 普通加法运算
y.add_(x)  # 原位加法运算
x[:, 1]  # 类似NumPy的索引方法
x.view(-1, 8)  # 改变向量的尺寸
x.item()  # 获取Tensor的标量值
```

> 更多的运算可以参考https://pytorch.org/docs/stable/torch.html

## Tensor与Numpy数组的相互转换

在PyTorch中，Tensor与NumPy array之间的转换时非常容易的，这是因为Tensor和NumPy array共享内存地址，改变其中一个将自动改变另外一个。

### 将Tensor转换为NumPy数组

使用Tensor的`.numpy()`方法可以方便地将Tensor转换为NumPy数组：

```python
a = torch.ones(5)  # 将a初始化为一个具有5个元素、全1的Tensor
print(a)
b = a.numpy()  # 进行转换
print(b)
```

由于Tensor和NumPy数组共享内存地址，改变其中一个，将自动改变另一个：

```python
a.add_(1)
print(a)
print(b)  # b随着a的改变而改变
```

### 将NumPy数组转换为Tensor

使用`torch.from_numpy()`方法将NumPy数组转换为Tensor：

```python
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)  # 进行转换
np.add(a, 1, out=a)
print(a)
print(b)  # a和b仍是同步改变的
```

总结一下，Tensor和Numpy数组采用以下方法进行转换：

```python
b = a.numpy()  # 将Tensor转换为np数组
y = torch.from_numpy(x)  # 将np数组转换为Tensor
```

## CUDA Tensor

在创建Tensor时给出device参数，可以直接在GPU创建Tensor；使用Tensor的`.to()`方法可以将其转换为CUDA Tensor，从而可以利用GPU进行运算：

```python
if torch.cuda.is_available():
	device = torch.device("cuda")  # CUDA device对象
	y = torch.ones_like(x, device=device)  # 在GPU创建Tensor
	x = x.to(device)  # 将Tensor转换为GPU Tensor
	z = x + y
	print(z)
	print(z.to("cpu", torch.double))  # to方法还可以改变dtype
```

创建GPU Tensor的方法总结如下：

- 创建Tensor时指定device设备为CUDA设备
- 使用Tensor的`.to()`方法迁移到GPU

```python
y = torch.one_like(x, device=torch.device("cuda"))  # 直接指定device在GPU创建Tensor
x = x.to(torch.device("cuda"))  # 将Tensor迁移到GPU
```

# 自动求导

在PyTorch中，进行一切关于神经网络的操作，都依赖于`autograd`子包。`autograd`包为Tensor的所有运算都提供了自动求导的机制，它是一个define-by-run的框架，这意味着，反向传播如何进行依赖于代码是如何构建的，每次迭代，具体的操作都是不同的。

## Tensor的自动求导

`torch.Tensor`是`autograd`包的核心类，如果设置`.requires_grad`为`True`，那么将跟踪Tensor的所有运算。如果完成了计算，那么调用`.backward()`可以自动地计算所有的梯度。Tensor的梯度将会累积在`.grad`属性中。

如果要停止跟踪一个Tensor，可以调用`.detach()`方法。如果不想自动求导，可以直接将代码放在`with torch.no_grad():`块中，这种方法在评估模型时特别有用。

在自动求导运行过程中，`Function`是很重要的概念。`Tensor`和`Function`紧密联系在一起，构成一个无环图，它记录了整个计算的历程。每个variable都有一个`.grad_fun`属性，指向创建了Tensor的`Function`（除非`grad_fun`设置为`None`）。

求导时只需调用Tensor的`.backward()`方法。如果`Tensor`是一个标量，不需要指定`backward()`的参数，但是如果`Tensor`有不止一个元素，那么需要指定`gradient`参数来进行维度匹配。

我们首先创建一个Tensor并设置`requires_grad=True`：

```python
import torch
x = torch.ones(2, 2, requires_grad=True)
print(x)
```

在Tensor上进行一次运算：

```python
y = x + 2
print(y)
```

由于`y`是通过一次运算创建的，那么它有一个`grad_fn`属性：

```python
print(y.grad_fn)
```

将返回一个运算对象名称及其所在的内存地址。

对`y`进行更多的运算：

```python
z = y * y * 3
out = z.mean()
print(z, out)
```

我们保留变量z和out，但暂时不进行更多操作。

此外，`.requires_grad_()`可以原位改变现有Tensor的`requires_grad`，如果不人工指定，默认为`False`。

```python
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)  # 返回False

a.requires_grad_(True)
print(a.requires_grad)  # 返回True

b = (a * a).sum()
print(b.grad_fn)
```

## 梯度

对`out`进行反向传播，由于是标量，所以不需要指定任何参数：

```python
out.backward()
```

然后我们查看out对x的梯度：

```python
print(x.grad)
```

返回的结果将会是所有元素为4.5的2x2矩阵。这里把out简写为`o`：

$$
\nabla{o}|_x=\frac{\mathrm{d}o}{\mathrm{d}x}=\mathrm{d}\left[\frac{1}{4}\sum_iz_i\right]/\mathrm{d}x=\mathrm{d}\left[\frac{1}{4}\sum_{i}\left[ 3\left(x_i+2\right)^2\right]\right]/\mathrm{d}x
$$

因此：

$$
\frac{\partial{o}}{\partial{x_i}}|_{x_i=1}=\frac{3}{2}\left(x_i+2\right)|_{x_i=1}=4.5
$$

稍作总结：Tensor创建时可指定其`requires_grad`参数为`True`来实现自动求导，并通过Tensor的`.backward()`进行反向传播。某个Tensor的梯度可通过它的`grad`属性查看。

```python
x = torch.ones(2, 2, requires_grad=True)  # 要求自动求导
out = x ^ 3
out.backward()  # 反向传播
x.grad  # 查看梯度
x.requires_grad_(True)  # 动态改变“是否需要自动求导”
with torch.no_grad():  # 以下操作不自动求导
    # do something
```

查看文档以获取更多信息：<http://pytorch.org/docs/autograd>

# 神经网络

神经网络可以通过`torch.nn`包来创建。`nn`基于`autograd`定义模型并求导，一个`nn.Module`包含层和方法`forward(input)`，该方法返回`output`。

典型的神经网络训练过程如下：

- 定义神经网络，其包含若干个可学习的参数
- 输入数据
- 神经网络处理数据
- 计算损失/误差
- 将梯度反向传播到神经网络的各个参数
- 更新网络权重

## 定义网络

让我们定义一个具有两个卷积层，三个全连接层的神经网络：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):  # 自定义一个神经网络类，继承自nn.Module类
    def __init__(self):
        super(Net, self).__init__()  # 执行父类的初始化方法
        self.conv1 = nn.Conv2d(1, 6, 5)  # 1个输入通道，6个输出通道，5x5卷积核
        self.conv2 = nn.Conv2d(6, 16, 5)  # 6个输入通道，16个输出通道，5x5卷积核
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 输入单元16x5x5，输出单元120
        self.fc2 = nn.Linear(120, 84)  # 输入单元120，输出单元84
        self.fc3 = nn.Linear(84, 10)  # 输入单元84，输出单元10

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))  # 第一个卷积层后最大池化，窗口为(2, 2)
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # 第二个卷积层后最大池化，窗口为2
        x = x.view(-1, self.num_flat_features(x))  # 变换Tensor尺寸，num_flat_features为自定义方法
        x = F.relu(self.fc1(x))  # 非线性化
        x = F.relu(self.fc2(x))  # 非线性化
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # 除了batch维度外的所有维度
        num_features = 1
        for s in size:
            num_features *= s  # 各个维度的乘积
        return num_features

net = Net()  # 创建一个Net对象
print(net)  # 查看网络结构
```

这里通过forward方法定义了前向传播的整个流程，而反向传播流程在调用`autograd`时自动地被定义。

可学习的参数可以通过`net.parameters()`返回：

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # 返回第一个卷积层的权重的尺寸([6, 1, 5, 5])
```

我们模拟一个32x32的输入，并用建立的神经网络前向传播获得计算结果：

```python
input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)  # 得到一个长度为10的向量
```

将所有参数的gradient buffers置零，并用随机梯度反向传播：

```python
net.zero_grad()
out.backward(torch.randn(1, 10))
```

> **注意**：
>
> `torch.nn`仅支持mini-batch，不支持单个样本输入。例如，`nn.Conv2d`的输入必需是一个4D的Tensor`nSamples x nChannels x Height x Width`。如果想输入单个样本，需要使用`input.unsqueeze(0)`来添加一个虚拟的batch维度。

让我们回顾一下到目前为止涉及的所有类：

- `torch.Tensor`：多维数组，支持自动求导运算（例如`backward()`），同时存储着Tensor的梯度
- `nn.Module`：神经网络模块
- `nn.Parameter`：一种Tensor，当分配为`Module`的属性时，自动注册为一个参数
- `autograd.Function`：执行自动求导操作的前向和反向定义，每个`Tensor`运算都创建至少一个`Function`节点，其与Tensor链接，并编码整个计算历程。

## 损失函数

损失函数的输入变量是(output, target)，输出output与target的差距。`torch.nn`定义了很多不同的损失函数，参见: https://pytorch.org/docs/stable/nn.html。一种简单的损失函数是`nn.MSELoss`，它计算output与target的均方误差。

计算损失函数的方法是先获取定义好的损失函数，然后使用它：

```python
output = net(input)
target = torch.arange(1, 11)  # 假定的target，仅做示例
target = target.view(1, -1)  # 使其与output具有相同形状
criterion = nn.MSELoss()  # 获取定义好的损失函数
loss = criterion(output, target)  # 计算损失函数
print(loss)
```

此时如果调用`.grad_fn`属性，将会看到计算图中增加了损失函数的计算。如此一来，整个计算图，包括损失函数，都将自动求导。

## 反向传播

为了反向传播误差，需要用到`loss.backward()`方法。我们需要首先清空现有的梯度缓存，否则梯度将会累积到现有的梯度中。

我们可以调用`loss.backward()`看一下conv1的偏置梯度：

```python
net.zero_grad()  # 清空所有参数的梯度缓存
print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()
print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
```

## 更新权重

最简单的更新权重的方法是随机梯度下降（SGD）：

weight = weight - learning_rate * gradient

我们可以使用简单的python语句实现它：

```python
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
```

然而，在神经网络中，我们希望使用多种不同的更新方法，包括Nesterov-SGD，Adam，RMSProp等等。这些方法都定义在`torch.optim`中，例如：

```python
import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr=0.01)  # 创建optimizer，制定优化参数和学习率
optimizer.zero_grad()  # 清空梯度缓存
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()  # optimizer更新权重
```

最后，我们总结一下神经网络中的重要操作：

```python
import torch
import torch.nn as nn  # 用于神经网络的各种操作
import torch.nn.functional as F  # 用于定义前向传播过程
import torch.optim as optim  # 用于创建optimizer以更新权重

# 关于网络结构定义
class Net(nn.Module):  # 自定义一个神经网络类，继承自nn.Module类
    def __init__(self):
        super(Net, self).__init__()  # 执行父类的初始化方法
        self.conv = nn.Conv2d(1, 6, 5)  # 创建卷积层，1个输入通道，6个输出通道，5x5卷积核
        self.fc = nn.Linear(6 * 5 * 5, 10)  # 创建全连接层，输入单元150，输出单元10

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv(x)), (2, 2))  # 卷积层后最大池化，窗口为(2, 2)
        x = x.view(-1, 150)  # 变换Tensor尺寸
        x = F.relu(self.fc(x))  # 非线性化，使用relu函数
        return x

net = Net()  # 创建一个Net对象
print(net)  # 查看网络结构

# 关于参数
params = list(net.parameters())  # net.parameters()用于获取所有可学习参数

# 前向和反向传播
out = net(input)  # 前向传播，计算结果
net.zero_grad()  # 清空梯度缓存
out.backward(torch.randn(1, 10))  # 反向传播

# 损失函数
criterion = nn.MSELoss()  # 创建一个损失计算函数
loss = criterion(output, target)  # 计算损失

# 优化器
optimizer = optim.SGD(net.parameters(), lr=0.01)  # 创建optimizer，制定优化参数和学习率
optimizer.step()  # optimizer更新权重
```

# 训练分类器

## 导入数据

当我们处理图像、文本、声音、视频数据时，可以使用python包导入数据，并转换为numpy数组，之后可将其转换成`torch.*Tensor`。

- 对于图像数据，可使用Pillow、OpenCV等包导入
- 对于音频数据，可使用scipy、librosa等包导入
- 对于文本数据，原生的Python、NLTK或者SpaCy都可以导入

为了方便地导入和查看数据，PyTorch提供了`torchvision`包，它可以导入ImageNet、CIFAR10、MNIST等常见的数据集，并提供数据转换器`torchvision.datasets`和`torch.utils.data.DataLoader`。

这里我们以CIFAR10数据集进行介绍，它包括了10个物体类别，每张图片尺寸为3个颜色通道，32x32像素。

## 训练一个图像分类器

我们将采取以下步骤训练这个分类器：

1. 使用`torchvision`加载和正则化CIFAR10训练及测试数据集
2. 定义卷积神经网络
3. 定义损失函数
4. 训练神经网络
5. 使用测试数据测试模型

### 1. 加载和正则化CIFAR10数据集

这里我们使用`torchvision`来加载数据。首先导入必要的模块：

```python
import torch
import torchvision
import torchvision.transforms as transforms
```

`torchvision`导入的数据为PILImage图像，范围为[0, 1]，我们可使用`torchvision.transforms`做适当变换，例如将范围调整为[-1, 1]。该模块的`Compose`方法可以将多种转换器组合在一起使用，这里我们使用了两种转换器：

- `torchvision.transforms.ToTensor`：将`PILImage`图像或者numpy数组转换为tensor
- `torchvision.transforms.Normalize`：进行正则化，应输入两个参数——每个通道的均值和标准差，正则化方法如下：
  $$input_i = \frac{input_i - mean_i}{std_i}$$

原范围为[0, 1]，如果各通道均值和标准差都设为0.5，那么变换后最小值为(0-0.5)/0.5=-1，最大值为(1-0.5)/0.5=1。

```python
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # 定义了一个数据转换器

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)  # 定义训练集（这里从网络上下载数据集）
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)  # 加载训练集，并定义batch_size，是否打乱等超参数

testset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)  # 定义测试集（这里从网络上下载数据集）
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
```

直观起见，我们可以查看各个类别的数据：

```python
import matplotlib.pyplot as plt
import numpy as np

def imshow(img):
    img = img / 2 + 0.5  # 去正则化，恢复到[0, 1]
    npimg = img.numpy()  # 将Tensor转换为numpy数组
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

# 随机获取一些图像
dataiter = iter(trainloader)  # 定义迭代器（trainloader的batch_size为4，所以每次获得4张图片）
images, labels = dataiter.next()  # 进行一次迭代（即随机获取一批图片）

imshow(torchvision.utils.make_grid(images))  # 显示图片，这里的make_grid自动把多个image合并成大的Tensor，并默认用深色底色填充空隙
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
```

### 2. 定义卷积神经网络

我们定义一个2卷积层、3全连接层的神经网络：

```python
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16*5*5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()
```

### 3. 定义损失函数和优化器

我们使用Cross-Entropy作为损失函数，使用SGD作为优化器：

```python
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```

### 4. 训练网络

我们在数据集上不断迭代，将数据集输入进网络中并优化：

```python
for epoch in range(2):  # 训练两个epoch
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):  # trainloader每次加载4张图片，enumerate的第二个参数指定了索引起始于0
        inputs, labels = data
        optimizer.zero_grad()
        output = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 2000 == 1999:  # 每2000个batch输出信息
            print('[%d, %d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
```

### 5. 测试模型

我们检查一下模型的分类效果。直观起见，我们显示几张测试集图片：

```python
dataiter = iter(testloader)
images, labels = dataiter.next()

# 显示图片
imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))
```

现在我们观察网络预测的结果：

```python
outputs = net(images)
_, predicted = torch.max(outputs, 1)
print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))
```

之后，我们对整个测试集进行测试，计算Accuracy：

```python
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
```

预测的Accuracy大约是53%。我们希望能找到那些类别分类效果比较差：

```python
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] == 1

for i in range(10):
    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))
```

## 利用GPU进行训练

利用GPU进行训练，需要首先定义CUDA设备：

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
```

接下来，将定义好的网络、输入、标签等等转移到GPU，这将把相关变量拷贝到GPU：

```python
net.to(device)
inputs, labels = inputs.to(device), labels.to(device)
```

# 并行处理

通过使用`DataParallel`能够方便地实现数据在GPU中的并行化处理。默认情况下，PyTorch仅使用一个GPU设备，但是我们可以通过以下方式实现多GPU并行处理：

```python
model = nn.DataParallel(model)
```

## 导入模块并定义参数

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

input_size = 5
output_size = 2
batch_size = 30
data_size = 100

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

## 数据集

我们定义一个随机的数据集：

```python
class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data= torch.randn(length, size)
    def __getitem__(self, index):
        return self.data[index]
    def __len__(self):
        return self.len
rand_loader = DataLoader(dataset=RandomDataset(input_size, 100),
                        batch_size=batch_size, shuffle=True)
```

## 定义模型

定义一个最简单的神经网络：

```python
class Model(nn.Module):
    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, output_size)
    def forward(self, input):
        output = self.fc(input)
        print("Input size:", input.size(), "Output size:", output.size())
        return output
```

## 创建模型和并行处理

要想使用多GPU，首先需要用`nn.DataParallel`将模型包装起来，之后用`model.to(device)`使模型能够在GPU上运行。

```python
model = Model(input_size, output_size)
if torch.cuda.device_count() > 1:
    print("Use", torch.cuda.device_count(), "GPUs!")
    model = nn.DataParallel(model)  # 使用DataParallel包装
model.to(device)  # 将模型置入GPU
```

## 运行模型

```python
for data in rand_loader:
    input = data.to(device)
    output = model(input)
    print("Outside: input size", input.size(),
         "output size", output.size())
```

如果你有多个GPU，那么就可以看到若干组输出了。
